import asyncio
import os
import json
import clickhouse_connect
from datetime import datetime, timedelta
from api_client import generate_plan_queries, fetch_and_save
from iris_parser import parse_plan_xml, parse_fchg_xml
from airflow.providers.postgres.hooks.postgres import PostgresHook

# --- ÐšÐžÐÐ¤Ð˜Ð“Ð£Ð ÐÐ¦Ð˜Ð¯ ---
def load_config():
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¹."""
    base_dir = "/opt/airflow/dags"
    config_path = os.path.join(base_dir, "config", "railway_config.json")
    
    print(f"ðŸ” Ð˜Ñ‰Ñƒ ÐºÐ¾Ð½Ñ„Ð¸Ð³ Ð·Ð´ÐµÑÑŒ: {config_path}")
    
    try:
        config_dir = os.path.join(base_dir, "config")
        if os.path.exists(config_dir):
            print(f"ðŸ“‚ Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¿Ð°Ð¿ÐºÐ¸ {config_dir}: {os.listdir(config_dir)}")
    except: pass

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                print(f"âœ… ÐšÐ¾Ð½Ñ„Ð¸Ð³ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {len(config.get('stations', {}))} ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¹")
                return config
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ JSON: {e}")
    
    return {
        "stations": {"8011160": "Berlin Hbf"}, 
        "monitored_types": []
    }

# --- HELPER: CLICKHOUSE CLIENT ---
def get_ch_client():
    return clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )

def ensure_clickhouse_tables():
    """Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð² ClickHouse ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½ÐµÑ‚."""
    client = get_ch_client()
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½ÑƒÐ¶Ð½Ð° Ð»Ð¸ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ñ train_delays Ð½Ð° ReplacingMergeTree
    try:
        result = client.query("SELECT engine FROM system.tables WHERE name = 'train_delays' AND database = 'default'")
        if result.result_rows:
            current_engine = result.result_rows[0][0]
            if 'ReplacingMergeTree' not in current_engine:
                print(f"âš  ÐœÐ¸Ð³Ñ€Ð°Ñ†Ð¸Ñ: train_delays Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ {current_engine}, Ð½ÑƒÐ¶ÐµÐ½ ReplacingMergeTree")
                print("ðŸ”„ ÐŸÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ train_delays...")
                client.command("DROP TABLE IF EXISTS train_delays")
    except Exception as e:
        print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´Ð²Ð¸Ð¶ÐºÐ°: {e}")
    
    # Silver layer: train_delays (ÑÑ‹Ñ€Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    # ReplacingMergeTree Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾ ORDER BY ÐºÐ»ÑŽÑ‡Ñƒ
    client.command("""
        CREATE TABLE IF NOT EXISTS train_delays (
            timestamp DateTime,
            city String,
            train_type String,
            train_id String,
            planned_departure DateTime,
            actual_departure DateTime,
            delay_in_min Int32,
            is_cancelled UInt8,
            origin String,
            destination String
        ) ENGINE = ReplacingMergeTree(timestamp)
        ORDER BY (city, train_id, planned_departure)
        PARTITION BY toYYYYMM(planned_departure)
    """)
    
    # Gold layer: daily_train_stats (Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ)
    client.command("""
        CREATE TABLE IF NOT EXISTS daily_train_stats (
            stat_date Date,
            city String,
            train_type String,
            total_trains UInt32,
            delayed_trains UInt32,
            avg_delay Float32,
            max_delay Int32,
            created_at DateTime
        ) ENGINE = ReplacingMergeTree(created_at)
        ORDER BY (stat_date, city, train_type)
        PARTITION BY toYYYYMM(stat_date)
    """)
    
    print("âœ… ClickHouse tables ensured: train_delays, daily_train_stats")

# --- Ð›ÐžÐ“Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• ---
def log_status(context, stage, status, msg=""):
    """ÐŸÐ¸ÑˆÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑƒÑ ÑÑ‚Ð°Ð¿Ð° Ð² ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ Ð¸ Ð² Postgres."""
    print(f"[{stage}] {status}: {msg}")
    
    try:
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        sql = """
            INSERT INTO api_ingestion_log (dag_id, execution_date, status, error_message)
            VALUES (%s, %s, %s, %s)
        """
        dag_id = str(context['dag'].dag_id)
        execution_date = str(context.get('execution_date', datetime.now()))
        
        pg_hook.run("""
            CREATE TABLE IF NOT EXISTS api_ingestion_log (
                run_id SERIAL PRIMARY KEY,
                dag_id VARCHAR(50),
                execution_date VARCHAR(50),
                status VARCHAR(20),
                error_message TEXT,
                created_at TIMESTAMP DEFAULT NOW()
            );
        """)
        
        pg_hook.run(sql, parameters=(dag_id, execution_date, status, f"{stage}: {msg}"))
    except Exception as e:
        print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð»Ð¾Ð³Ð° Ð² Postgres: {e}")

# ==========================================
# 1. EXTRACT DATA (API -> Parquet/Bronze)
# ==========================================
async def extract_data(config):
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð´Ð²ÑƒÑ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²:
    1. /plan/{evaNo}/{YYMMDD}/{HH} - Ð’Ð¡Ð• Ð·Ð°Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ÐµÐ·Ð´Ð°
    2. /fchg/{evaNo} - Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ (Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸, Ð¾Ñ‚Ð¼ÐµÐ½Ñ‹)
    
    ÐŸÐ¾Ñ‚Ð¾Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÑŽÑ‚ÑÑ Ð² load_to_silver.
    """
    from api_client import generate_plan_queries, generate_fchg_queries, fetch_and_save
    
    stations = config.get("stations", {})
    hours_back = config.get("hours_back", 24)
    hours_forward = config.get("hours_forward", 0)
    
    # 1. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ Ð¿Ð»Ð°Ð½Ð° (Ð²ÑÐµ Ð¿Ð¾ÐµÐ·Ð´Ð°)
    plan_queries = generate_plan_queries(
        stations=stations,
        hours_back=hours_back,
        hours_forward=hours_forward,
    )
    
    # 2. Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð´Ð»Ñ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹ (Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸)
    fchg_queries = generate_fchg_queries(stations=stations)
    
    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹
    all_queries = plan_queries + fchg_queries
    
    print(f"ðŸŒ TASK 1: EXTRACT. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…:")
    print(f"   - Ð¡Ñ‚Ð°Ð½Ñ†Ð¸Ð¹: {len(stations)}")
    print(f"   - ÐŸÐ»Ð°Ð½ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {len(plan_queries)} (Ñ‡Ð°ÑÑ‹: -{hours_back} / +{hours_forward})")
    print(f"   - Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {len(fchg_queries)}")
    print(f"   - Ð’ÑÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {len(all_queries)}")
    
    return await fetch_and_save(
        queries=all_queries,
        output_path="/opt/airflow/data/raw_api_data",
        max_concurrent=10,
        rate_limit=60,
    )

# ==========================================
# 2. LOAD TO SILVER (Parquet -> ClickHouse Raw)
# ==========================================
def load_to_silver(df, config):
    """
    ÐŸÐ°Ñ€ÑÐ¸Ñ‚ XML Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð² ClickHouse.
    
    Ð›Ð¾Ð³Ð¸ÐºÐ° Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ:
    1. ÐŸÐ°Ñ€ÑÐ¸Ð¼ /plan Ð´Ð°Ð½Ð½Ñ‹Ðµ -> Ð±Ð°Ð·Ð¾Ð²Ð¾Ðµ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ (delay=0)
    2. ÐŸÐ°Ñ€ÑÐ¸Ð¼ /fchg Ð´Ð°Ð½Ð½Ñ‹Ðµ -> Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°Ð¼Ð¸
    3. ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼: fchg Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÑŽÑ‚ plan Ð¿Ð¾ ÐºÐ»ÑŽÑ‡Ñƒ (train_id, planned_departure, city)
    """
    print("ðŸ“¥ TASK 2: LOAD TO SILVER...")
    
    target_types = set(config.get("monitored_types", []))
    
    # Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…: ÐºÐ»ÑŽÑ‡ -> Ð´Ð°Ð½Ð½Ñ‹Ðµ
    # ÐšÐ»ÑŽÑ‡: (train_id, planned_departure, city)
    trains_dict = {}
    
    plan_count = 0
    fchg_count = 0
    error_count = 0
    
    for _, row in df.iterrows():
        if row['error']:
            error_count += 1
            continue
            
        if not row['response_data']:
            continue
        
        station_name = row.get('station_name', 'Unknown')
        query_type = row.get('query_type', 'plan')
        
        try:
            # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ð°Ñ€ÑÐµÑ€ Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ‚Ð¸Ð¿Ð° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
            if query_type == 'fchg':
                rows = parse_fchg_xml(row['response_data'], station_name)
                fchg_count += len(rows)
            else:
                rows = parse_plan_xml(row['response_data'], station_name)
                plan_count += len(rows)
            
            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ñ‚Ð¸Ð¿Ñƒ Ð¿Ð¾ÐµÐ·Ð´Ð° ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
            if target_types:
                rows = [r for r in rows if r[2] in target_types]
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ
            for row_data in rows:
                # row_data: (timestamp, city, train_type, train_id, planned_departure, 
                #            actual_departure, delay_in_min, is_cancelled, origin, destination)
                key = (row_data[3], row_data[4], row_data[1])  # (train_id, planned_departure, city)
                
                # fchg Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð¼ÐµÑŽÑ‚ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ (Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÑŽÑ‚ plan)
                if query_type == 'fchg':
                    trains_dict[key] = row_data
                elif key not in trains_dict:
                    # plan Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÐµÑ‰Ñ‘ Ð½ÐµÑ‚ Ð·Ð°Ð¿Ð¸ÑÐ¸
                    trains_dict[key] = row_data
            
        except Exception as e:
            print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð´Ð»Ñ {station_name}: {e}")
            error_count += 1
    
    print(f"ðŸ“Š ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½:")
    print(f"   - Plan Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {plan_count}")
    print(f"   - Fchg Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (Ñ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸ÑÐ¼Ð¸): {fchg_count}")
    print(f"   - ÐžÑˆÐ¸Ð±Ð¾Ðº: {error_count}")
    print(f"   - Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ð¾ÐµÐ·Ð´Ð¾Ð² Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ: {len(trains_dict)}")
    
    if not trains_dict:
        print("âš  LOAD: ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸.")
        return 0

    # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº
    all_parsed = list(trains_dict.values())
    
    # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°Ð¼
    delayed_count = sum(1 for r in all_parsed if r[6] > 0)
    cancelled_count = sum(1 for r in all_parsed if r[7] == 1)
    print(f"   - Ð¡ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹: {delayed_count}")
    print(f"   - ÐžÑ‚Ð¼ÐµÐ½ÐµÐ½Ð¾: {cancelled_count}")

    client = get_ch_client()
    
    client.insert('train_delays', all_parsed, 
                  column_names=['timestamp', 'city', 'train_type', 'train_id', 
                                'planned_departure', 'actual_departure', 
                                'delay_in_min', 'is_cancelled', 'origin', 'destination'])
    
    print(f"âœ… LOAD: Ð’ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð¾ {len(all_parsed)} ÑÑ‚Ñ€Ð¾Ðº Ð² Silver ÑÐ»Ð¾Ð¹ (train_delays).")
    return len(all_parsed)

# ==========================================
# 3. DATA QUALITY CHECK (Validation)
# ==========================================
def data_quality_check():
    print("ðŸ§ TASK 3: DATA QUALITY CHECK...")
    client = get_ch_client()
    
    # ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ (Ñ€Ð¾Ð½ÑÑŽÑ‚ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½)
    critical_checks = [
        ("Null Check: Train IDs", 
         "SELECT count() FROM train_delays FINAL WHERE train_id = '' AND timestamp > now() - INTERVAL 1 HOUR"),
         
        ("Null Check: Cities", 
         "SELECT count() FROM train_delays FINAL WHERE city = '' AND timestamp > now() - INTERVAL 1 HOUR"),

        ("Range Check: Negative Delays", 
         "SELECT count() FROM train_delays FINAL WHERE delay_in_min < -60"),
         
        ("Range Check: Extreme Delays (>1000 min)", 
         "SELECT count() FROM train_delays FINAL WHERE delay_in_min > 1000"),
         
        ("Range Check: Future Data (>7 Days)", 
         "SELECT count() FROM train_delays FINAL WHERE planned_departure > now() + INTERVAL 7 DAY"),

        ("Ref Integrity: Unknown Stations", 
         "SELECT count() FROM train_delays FINAL WHERE city = 'Unknown' AND timestamp > now() - INTERVAL 1 HOUR"),
    ]
    
    # ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ (Ð½Ðµ Ñ€Ð¾Ð½ÑÑŽÑ‚ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½)
    warning_checks = [
        ("Duplicates (pre-merge)",
         """SELECT count() FROM (
             SELECT train_id, planned_departure, city, count() as cnt 
             FROM train_delays
             WHERE timestamp > now() - INTERVAL 1 HOUR
             GROUP BY train_id, planned_departure, city 
             HAVING cnt > 1
         )"""),
    ]
    
    failed_checks = []
    
    for check_name, sql in critical_checks:
        try:
            result = client.query(sql).result_rows[0][0]
            if result > 0:
                msg = f"âŒ DQ FAIL: {check_name} -> Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {result} Ð¿Ð»Ð¾Ñ…Ð¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"
                print(msg)
                failed_checks.append(msg)
            else:
                print(f"âœ… DQ PASS: {check_name}")
        except Exception as e:
            print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ {check_name}: {e}")
            failed_checks.append(f"SQL Error in {check_name}: {e}")
    
    for check_name, sql in warning_checks:
        try:
            result = client.query(sql).result_rows[0][0]
            if result > 0:
                print(f"âš  DQ WARNING: {check_name} -> {result} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (Ð±ÑƒÐ´ÑƒÑ‚ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹)")
            else:
                print(f"âœ… DQ PASS: {check_name}")
        except Exception as e:
            print(f"âš  ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ {check_name}: {e}")
    
    try:
        client.command("OPTIMIZE TABLE train_delays FINAL")
        print("âœ… OPTIMIZE TABLE train_delays FINAL - Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°")
    except Exception as e:
        print(f"âš  OPTIMIZE Ð½Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½: {e}")
            
    if failed_checks:
        raise Exception(f"Data Quality Checks Failed:\n" + "\n".join(failed_checks))

# ==========================================
# 4. TRANSFORM GOLD (Silver -> Aggregated)
# ==========================================
def transform_gold():
    print("ðŸ”¨ TASK 4: TRANSFORM GOLD...")
    client = get_ch_client()
    
    # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚ Deutsche Bahn: Ð¿Ð¾ÐµÐ·Ð´ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑ‚ÑÑ Ð¾Ð¿Ð¾Ð·Ð´Ð°Ð²ÑˆÐ¸Ð¼ ÐµÑÐ»Ð¸ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ° > 5 Ð¼Ð¸Ð½ÑƒÑ‚
    query = """
    INSERT INTO daily_train_stats
    SELECT
        toDate(planned_departure) as stat_date,
        city,
        train_type,
        count() as total_trains,
        countIf(delay_in_min > 5) as delayed_trains,
        avgIf(delay_in_min, delay_in_min > 5) as avg_delay,
        maxIf(delay_in_min, delay_in_min > 5) as max_delay,
        now() as created_at
    FROM train_delays FINAL
    WHERE planned_departure >= toStartOfDay(now() - INTERVAL 1 DAY)
      AND planned_departure < toStartOfDay(now() + INTERVAL 1 DAY)
    GROUP BY stat_date, city, train_type
    """
    
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð¸ Ð²Ñ‡ÐµÑ€Ð° Ð¿ÐµÑ€ÐµÐ´ Ð²ÑÑ‚Ð°Ð²ÐºÐ¾Ð¹
    client.command("""
        ALTER TABLE daily_train_stats DELETE 
        WHERE stat_date >= toDate(now() - INTERVAL 1 DAY)
    """)
    
    client.command(query)
    print("âœ… TRANSFORM: Gold ÑÐ»Ð¾Ð¹ (daily_train_stats) Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½.")

# --- ORCHESTRATOR ---
async def run_pipeline(context):
    config = load_config()
    
    # 0. ENSURE TABLES EXIST
    try:
        ensure_clickhouse_tables()
    except Exception as e:
        log_status(context, "INIT", "FAILED", f"Cannot create tables: {e}")
        raise
    
    # 1. EXTRACT
    try:
        df = await extract_data(config)
        
        # Tech Check: API health
        total = len(df)
        failed_count = df['error'].notna().sum()
        
        if total == 0:
            raise Exception("CRITICAL: ÐÐµ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.")
            
        if failed_count == total:
            raise Exception("CRITICAL: Ð’ÑÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº API ÑƒÐ¿Ð°Ð»Ð¸.")
            
        success_rate = (total - failed_count) / total * 100
        print(f"ðŸ“ˆ API Success Rate: {success_rate:.1f}% ({total - failed_count}/{total})")
        
        if failed_count > 0:
            print(f"âš  WARNING: {failed_count}/{total} Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ¾Ð¹.")
            
    except Exception as e:
        log_status(context, "EXTRACT", "FAILED", str(e))
        raise

    # 2. LOAD
    try:
        count = load_to_silver(df, config)
    except Exception as e:
        log_status(context, "LOAD", "FAILED", str(e))
        raise

    # 3. DQ CHECK
    if count > 0:
        try:
            data_quality_check()
        except Exception as e:
            log_status(context, "DQ_CHECK", "FAILED", str(e))
            raise

        # 4. TRANSFORM
        try:
            transform_gold()
        except Exception as e:
            log_status(context, "TRANSFORM", "FAILED", str(e))
            raise
            
    log_status(context, "PIPELINE", "SUCCESS", f"Processed {count} records")

def main(**kwargs):
    asyncio.run(run_pipeline(kwargs))