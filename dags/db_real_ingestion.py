# db_real_ingestion.py

import asyncio
import os
import json
import clickhouse_connect
from datetime import datetime, timedelta
from api_client import fetch_and_save
from iris_parser import parse_db_xml
from airflow.providers.postgres.hooks.postgres import PostgresHook

# --- –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –ö–û–ù–§–ò–ì–ê ---
def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å—Ç–∞–Ω—Ü–∏–π –∏ —Ç–∏–ø–æ–≤ –ø–æ–µ–∑–¥–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    base_dir = "/opt/airflow/dags"
    config_path = os.path.join(base_dir, "config", "railway_config.json")
    
    print(f"üîç –ò—â—É –∫–æ–Ω—Ñ–∏–≥ –∑–¥–µ—Å—å: {config_path}")

    # === –û–¢–õ–ê–î–ö–ê (DEBUG) ===
    try:
        config_dir = os.path.join(base_dir, "config")
        if os.path.exists(config_dir):
            print(f"üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ {config_dir}: {os.listdir(config_dir)}")
        else:
            print(f"‚ùå –ü–∞–ø–∫–∞ {config_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            print(f"üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–æ—Ä–Ω—è {base_dir}: {os.listdir(base_dir)}")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ –ø—É—Ç–µ–π: {e}")
    # =======================

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {len(config.get('stations', {}))} —Å—Ç–∞–Ω—Ü–∏–π")
                return config
        except Exception as e:
            print(f"‚ùå –§–∞–π–ª –µ—Å—Ç—å, –Ω–æ –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON: {e}")
    else:
        print("‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ —ç—Ç–æ–º—É –ø—É—Ç–∏.")

    # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
    print("‚ö† –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (12 –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π).")
    return {
        "stations": {
            "8011160": "Berlin Hbf",
            "8000207": "K√∂ln Hbf",
            "8000261": "M√ºnchen Hbf",
            "8000105": "Frankfurt (Main) Hbf",
            "8002549": "Hamburg Hbf",
            "8000096": "Stuttgart Hbf",
            "8000244": "Mannheim Hbf",
            "8000191": "Karlsruhe Hbf",
            "8000284": "N√ºrnberg Hbf",
            "8000152": "Hannover Hbf",
            "8000080": "Dortmund Hbf",
            "8000260": "W√ºrzburg Hbf"
        },
        "monitored_types": ["ICE", "IC", "EC", "ECE", "RE", "RB", "S", "NX", "FLX", "NJ"],
        "hours_to_fetch": 6  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–≤–∫–ª—é—á–∞—è —Ç–µ–∫—É—â–∏–π)
    }


# --- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í POSTGRES ---
def log_ingestion_status(context, status, records_count, stations_count=0, error_message=None):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Postgres."""
    try:
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        create_sql = """
        CREATE TABLE IF NOT EXISTS api_ingestion_log (
            run_id SERIAL PRIMARY KEY,
            dag_id VARCHAR(50),
            execution_date TIMESTAMP,
            status VARCHAR(20),
            records_count INT,
            stations_count INT,
            error_message TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        );
        """
        pg_hook.run(create_sql)

        insert_sql = """
            INSERT INTO api_ingestion_log 
            (dag_id, execution_date, status, records_count, stations_count, error_message)
            VALUES (%s, %s, %s, %s, %s, %s);
        """
        
        dag_id = str(context['dag'].dag_id)
        execution_date = str(context.get('execution_date', datetime.now()))
        
        pg_hook.run(insert_sql, parameters=(
            dag_id, execution_date, status, records_count, stations_count, error_message
        ))
        print(f"üìù –°—Ç–∞—Ç—É—Å '{status}' –∑–∞–ø–∏—Å–∞–Ω –≤ Postgres (–∑–∞–ø–∏—Å–µ–π: {records_count}).")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞ –≤ Postgres: {e}")


def build_plan_queries(stations: dict, hours_to_fetch: int = 6) -> list:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ /plan/{evaNo}/{date}/{hour} endpoint.
    
    Args:
        stations: –°–ª–æ–≤–∞—Ä—å {eva_id: station_name}
        hours_to_fetch: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–Ω–∞—á–∏–Ω–∞—è —Å —Ç–µ–∫—É—â–µ–≥–æ)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å URL –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    """
    queries = []
    now = datetime.now()
    
    for eva_id, station_name in stations.items():
        for hour_offset in range(hours_to_fetch):
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤–æ–µ –≤—Ä–µ–º—è
            target_time = now + timedelta(hours=hour_offset)
            
            # –§–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: YYMMDD
            date_str = target_time.strftime('%y%m%d')
            # –§–æ—Ä–º–∞—Ç —á–∞—Å–∞: HH (—Å –≤–µ–¥—É—â–∏–º –Ω—É–ª—ë–º)
            hour_str = target_time.strftime('%H')
            
            queries.append({
                "url": f"https://apis.deutschebahn.com/db-api-marketplace/apis/timetables/v1/plan/{eva_id}/{date_str}/{hour_str}",
                "params": {},
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                "_meta": {
                    "eva_id": eva_id,
                    "station_name": station_name,
                    "date": date_str,
                    "hour": hour_str,
                    "type": "plan"
                }
            })
    
    return queries


def build_fchg_queries(stations: dict) -> list:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ /fchg/{evaNo} endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è 
    –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π (–∑–∞–¥–µ—Ä–∂–∫–∏, –æ—Ç–º–µ–Ω—ã, –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–ª–∞—Ç—Ñ–æ—Ä–º).
    
    Args:
        stations: –°–ª–æ–≤–∞—Ä—å {eva_id: station_name}
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å URL –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    queries = []
    
    for eva_id, station_name in stations.items():
        queries.append({
            "url": f"https://apis.deutschebahn.com/db-api-marketplace/apis/timetables/v1/fchg/{eva_id}",
            "params": {},
            "_meta": {
                "eva_id": eva_id,
                "station_name": station_name,
                "type": "fchg"
            }
        })
    
    return queries


# --- –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê ---
async def run_real_ingestion(context):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
    1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥ —Å–æ —Å—Ç–∞–Ω—Ü–∏—è–º–∏
    2. –§–æ—Ä–º–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∫ API:
       - /plan/{evaNo}/{date}/{hour} ‚Äî –ø–ª–∞–Ω–æ–≤–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤
       - /fchg/{evaNo} ‚Äî –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    3. –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ API
    4. –ü–∞—Ä—Å–∏—Ç XML –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ ClickHouse
    """
    config = load_config()
    stations = config.get("stations", {})
    hours_to_fetch = config.get("hours_to_fetch", 6)
    fetch_realtime_changes = config.get("fetch_realtime_changes", True)
    
    # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–∏–ø—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ set
    target_types = set(filter(None, config.get("monitored_types", [])))
    
    if not stations:
        error_msg = "CRITICAL: No stations configured!"
        log_ingestion_status(context, 'FAILED', 0, 0, error_msg)
        raise Exception(error_msg)
    
    # === –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ó–ê–ü–†–û–°–û–í –ö API ===
    print(f"\n{'='*60}")
    print(f"üöÇ DEUTSCHE BAHN TIMETABLE INGESTION")
    print(f"{'='*60}")
    print(f"üìç –°—Ç–∞–Ω—Ü–∏–π: {len(stations)}")
    print(f"‚è∞ –ß–∞—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {hours_to_fetch}")
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ real-time –∏–∑–º–µ–Ω–µ–Ω–∏–π: {'–î–∞' if fetch_realtime_changes else '–ù–µ—Ç'}")
    if target_types:
        print(f"üéØ –§–∏–ª—å—Ç—Ä —Ç–∏–ø–æ–≤ –ø–æ–µ–∑–¥–æ–≤: {', '.join(sorted(target_types))}")
    else:
        print(f"üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–∏–ø–æ–≤: –æ—Ç–∫–ª—é—á–µ–Ω–∞ (–≤—Å–µ –ø–æ–µ–∑–¥–∞)")
    print(f"{'='*60}\n")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –∫ /plan/ endpoint
    plan_queries = build_plan_queries(stations, hours_to_fetch)
    print(f"üìã –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(plan_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ /plan/ endpoint")
    print(f"   ({len(stations)} —Å—Ç–∞–Ω—Ü–∏–π √ó {hours_to_fetch} —á–∞—Å–æ–≤)")
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –∫ /fchg/ –¥–ª—è real-time –¥–∞–Ω–Ω—ã—Ö
    fchg_queries = []
    if fetch_realtime_changes:
        fchg_queries = build_fchg_queries(stations)
        print(f"üìã –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(fchg_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ /fchg/ endpoint")
    
    all_queries = plan_queries + fchg_queries
    print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API: {len(all_queries)}")
    
    output_path = "/opt/airflow/data/raw_api_data"
    
    # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó API ===
    print(f"\nüåê –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ API...")
    
    df = await fetch_and_save(
        queries=all_queries,
        output_path=output_path,
        max_concurrent=5,   # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º concurrency –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        rate_limit=60       # DB API limit: 60 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É
    )

    # === –ü–†–û–í–ï–†–ö–ê –ù–ê –û–®–ò–ë–ö–ò ===
    failed_requests = df['error'].notna().sum()
    total_requests = len(all_queries)
    successful_requests = total_requests - failed_requests
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ API –∑–∞–ø—Ä–æ—Å–æ–≤:")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö: {successful_requests}")
    print(f"   ‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö: {failed_requests}")
    print(f"   üìà Success rate: {successful_requests/total_requests*100:.1f}%")

    # –ï—Å–ª–∏ –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã —É–ø–∞–ª–∏ - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
    if failed_requests == total_requests and total_requests > 0:
        error_msg = f"CRITICAL: All {total_requests} API requests failed."
        log_ingestion_status(context, 'FAILED', 0, len(stations), error_msg)
        raise Exception(error_msg)

    # –ï—Å–ª–∏ —á–∞—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤ —É–ø–∞–ª–∞ - –ª–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏
    if failed_requests > 0:
        print(f"\n‚ö† –î–µ—Ç–∞–ª–∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
        for _, row in df[df['error'].notna()].iterrows():
            print(f"   - {row['url']}: {row['error']}")

    # === –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö CLICKHOUSE ===
    client = clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        port=8123,
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )

    # === –ü–ê–†–°–ò–ù–ì –ò –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –î–ê–ù–ù–´–• ===
    print(f"\nüîÑ –ü–∞—Ä—Å–∏–Ω–≥ XML –æ—Ç–≤–µ—Ç–æ–≤...")
    
    all_parsed_data = []
    stations_with_data = set()
    plan_records = 0
    fchg_records = 0
    
    for _, row in df.iterrows():
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–ø–∞–≤—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        if row['error']:
            continue

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ URL
        url_parts = row['url'].split('/')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º EVA ID
        if '/plan/' in row['url']:
            # URL: .../plan/{evaNo}/{date}/{hour}
            eva_id = url_parts[-3]
            request_type = "plan"
            date_hour = f"{url_parts[-2]}/{url_parts[-1]}"
        elif '/fchg/' in row['url']:
            # URL: .../fchg/{evaNo}
            eva_id = url_parts[-1]
            request_type = "fchg"
            date_hour = "realtime"
        else:
            print(f"  ‚ö† –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL: {row['url']}")
            continue
        
        city = stations.get(eva_id, "Unknown")
        
        if row['response_data']:
            # === –û–¢–õ–ê–î–ö–ê: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é XML ===
            xml_length = len(row['response_data'])
            if xml_length > 0:
                xml_preview = row['response_data'][:300] if xml_length > 300 else row['response_data']
                # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
                xml_preview_clean = ' '.join(xml_preview.split())
                print(f"\n  üìÑ {city} [{request_type}] ({date_hour}):")
                print(f"     XML size: {xml_length} chars")
                print(f"     Preview: {xml_preview_clean[:150]}...")
            
            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
            parsed_rows = parse_db_xml(row['response_data'], city)
            
            if parsed_rows:
                print(f"     Parsed: {len(parsed_rows)} –∑–∞–ø–∏—Å–µ–π")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ —Ç–∏–ø–∞–º –ø–æ–µ–∑–¥–æ–≤
                if target_types:
                    before_filter = len(parsed_rows)
                    parsed_rows = [r for r in parsed_rows if r[2] in target_types]
                    after_filter = len(parsed_rows)
                    if before_filter != after_filter:
                        print(f"     Filtered: {before_filter} ‚Üí {after_filter} –∑–∞–ø–∏—Å–µ–π")
                
                if parsed_rows:
                    all_parsed_data.extend(parsed_rows)
                    stations_with_data.add(city)
                    
                    if request_type == "plan":
                        plan_records += len(parsed_rows)
                    else:
                        fchg_records += len(parsed_rows)
            else:
                print(f"     ‚ö† –ü–∞—Ä—Å–µ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã—Ö")

    total_records = len(all_parsed_data)
    
    print(f"\n{'='*60}")
    print(f"üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –°—Ç–∞–Ω—Ü–∏–π –∑–∞–ø—Ä–æ—à–µ–Ω–æ: {len(stations)}")
    print(f"   –°—Ç–∞–Ω—Ü–∏–π —Å –¥–∞–Ω–Ω—ã–º–∏: {len(stations_with_data)}")
    print(f"   –ó–∞–ø–∏—Å–µ–π –∏–∑ /plan/: {plan_records}")
    print(f"   –ó–∞–ø–∏—Å–µ–π –∏–∑ /fchg/: {fchg_records}")
    print(f"   –í–°–ï–ì–û –∑–∞–ø–∏—Å–µ–π: {total_records}")
    if target_types:
        print(f"   –¢–∏–ø—ã –ø–æ–µ–∑–¥–æ–≤: {', '.join(sorted(target_types))}")
    print(f"{'='*60}\n")

    # === –ó–ê–ì–†–£–ó–ö–ê –í CLICKHOUSE ===
    if all_parsed_data:
        try:
            client.insert(
                'train_delays', 
                all_parsed_data, 
                column_names=[
                    'timestamp', 'city', 'train_type', 'train_id', 
                    'planned_departure', 'actual_departure', 
                    'delay_in_min', 'is_cancelled',
                    'origin', 'destination'
                ]
            )
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_records} —Å—Ç—Ä–æ–∫ –≤ ClickHouse.")
            log_ingestion_status(context, 'SUCCESS', total_records, len(stations_with_data))
        except Exception as e:
            error_msg = f"Failed to insert into ClickHouse: {str(e)}"
            print(f"‚ùå {error_msg}")
            log_ingestion_status(context, 'FAILED', 0, len(stations_with_data), error_msg)
            raise
    else:
        warning_msg = "API –æ—Ç–≤–µ—Ç–∏–ª —É—Å–ø–µ—à–Ω–æ, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç (–∏–ª–∏ –≤—Å–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã)."
        print(f"‚ö† {warning_msg}")
        log_ingestion_status(context, 'WARNING', 0, len(stations_with_data), warning_msg)


def main(**kwargs):
    """Entry point –¥–ª—è Airflow DAG."""
    asyncio.run(run_real_ingestion(kwargs))