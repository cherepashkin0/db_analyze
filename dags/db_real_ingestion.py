# db_real_ingestion.py

import asyncio
import os
import clickhouse_connect
from api_client import fetch_and_save
from iris_parser import parse_db_xml
from airflow.providers.postgres.hooks.postgres import PostgresHook
# from airflow.utils.email import send_email

# –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ EVA ID
STATIONS = {
    "8011160": "Berlin Hbf",
    "8000207": "K√∂ln Hbf",
    "8000261": "M√ºnchen Hbf",
    "8000105": "Frankfurt (Main) Hbf",
    "8002549": "Hamburg Hbf",
    "8000096": "Stuttgart Hbf",
    "8000244": "Mannheim Hbf",
    "8000191": "Karlsruhe Hbf",
    "8000284": "N√ºrnberg Hbf",
    "8000152": "Hannover Hbf",
    "8000080": "Dortmund Hbf",
    "8000260": "W√ºrzburg Hbf"
}

# --- –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ---
def log_ingestion_status(context, status, records_count, error_message=None):
    """–ü–∏—à–µ—Ç —Å—Ç–∞—Ç—É—Å (SUCCESS/FAILED) –≤ Postgres"""
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –µ—â–µ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    create_sql = """
    CREATE TABLE IF NOT EXISTS api_ingestion_log (
        run_id SERIAL PRIMARY KEY,
        dag_id VARCHAR(50),
        execution_date VARCHAR(50),
        status VARCHAR(20),
        records_count INT,
        error_message TEXT,
        created_at TIMESTAMP DEFAULT NOW()
    );
    """
    pg_hook.run(create_sql)

    insert_sql = """
        INSERT INTO api_ingestion_log (dag_id, execution_date, status, records_count, error_message)
        VALUES (%s, %s, %s, %s, %s);
    """
    
    dag_id = str(context['dag'].dag_id)
    execution_date = str(context['execution_date'])
    
    pg_hook.run(insert_sql, parameters=(dag_id, execution_date, status, records_count, error_message))
    print(f"üìù –°—Ç–∞—Ç—É—Å '{status}' –∑–∞–ø–∏—Å–∞–Ω –≤ Postgres.")

# -------------------------------------------

async def run_real_ingestion(context):
    queries = [
        {"url": f"https://apis.deutschebahn.com/db-api-marketplace/apis/timetables/v1/fchg/{eva}"}
        for eva in STATIONS.keys()
    ]
    
    output_path = "/opt/airflow/data/raw_api_data"
    
    df = await fetch_and_save(
        queries=queries,
        output_path=output_path,
        max_concurrent=3,
        rate_limit=60
    )

    # === –ë–õ–û–ö –ü–†–û–í–ï–†–ö–ò –ù–ê –û–®–ò–ë–ö–ò ===
    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–µ—Ä–Ω—É–ª–∏ –æ—à–∏–±–∫—É (–∫–æ–ª–æ–Ω–∫–∞ error –Ω–µ –ø—É—Å—Ç–∞—è)
    failed_requests = df['error'].notna().sum()
    total_requests = len(queries)
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {total_requests - failed_requests}/{total_requests} —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.")

    # –ï—Å–ª–∏ 100% –∑–∞–ø—Ä–æ—Å–æ–≤ —É–ø–∞–ª–∏ - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
    if failed_requests == total_requests:
        error_msg = f"CRITICAL: All {total_requests} API requests failed. Check logs for details."
        
        # 1. –ü–∏—à–µ–º FAIL –≤ Postgres
        log_ingestion_status(context, 'FAILED', 0, error_msg)
        
        # 2. –ë–†–û–°–ê–ï–ú –ò–°–ö–õ–Æ–ß–ï–ù–ò–ï -> –≠—Ç–æ –≤—ã–∑–æ–≤–µ—Ç on_failure_callback (Telegram)
        raise Exception(error_msg)
    # ===============================

    # 2. –ï—Å–ª–∏ –º—ã –∑–¥–µ—Å—å, –∑–Ω–∞—á–∏—Ç —Ö–æ—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å
    client = clickhouse_connect.get_client(
        host=os.getenv('CLICKHOUSE_HOST', 'clickhouse'),
        username=os.getenv('CLICKHOUSE_USER', 'default'),
        password=os.getenv('CLICKHOUSE_PASSWORD')
    )

    all_parsed_data = []
    
    for _, row in df.iterrows():
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—à–∏–±–∫–∞–º–∏
        if row['error']:
            continue

        eva_id = row['url'].split('/')[-1]
        city = STATIONS.get(eva_id, "Unknown")
        
        if row['response_data']:
            parsed_rows = parse_db_xml(row['response_data'], city)
            all_parsed_data.extend(parsed_rows)

    count = len(all_parsed_data)

    if all_parsed_data:
        client.insert('train_delays', all_parsed_data, 
                        column_names=[
                            'timestamp', 'city', 'train_type', 'train_id', 
                            'planned_departure', 'actual_departure', 
                            'delay_in_min', 'is_cancelled'
                        ])
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {count} —Å—Ç—Ä–æ–∫.")
    else:
        print("‚ö† API –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–¥–µ—Ä–∂–∫–∞—Ö –Ω–µ—Ç.")
        # 3. –ü–∏—à–µ–º SUCCESS –≤ Postgres
        log_ingestion_status(context, 'SUCCESS', count)

    # log_success_to_postgres(context, count)


# def log_success_to_postgres(context, records_count):
#     """–ü–∏—à–µ—Ç –ª–æ–≥ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Postgres"""
#     pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
#     sql = """
#         INSERT INTO api_ingestion_log (dag_id, execution_date, status, records_count)
#         VALUES (%s, %s, 'SUCCESS', %s);
#     """
    
#     # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ ---
#     # –ú—ã –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ–±—ä–µ–∫—Ç—ã Airflow –≤ —Å—Ç—Ä–æ–∫–∏.
#     # –≠—Ç–æ —Å–Ω–∏–º–∞–µ—Ç –æ–±–µ—Ä—Ç–∫—É 'Proxy' –∏ –¥—Ä–∞–π–≤–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—É—á–∞–µ—Ç –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç.
    
#     dag_id = str(context['dag'].dag_id)
#     execution_date = str(context['execution_date']) # –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç –¥–∞—Ç—É –≤ ISO-—Å—Ç—Ä–æ–∫—É
    
#     # -------------------------
    
#     pg_hook.run(sql, parameters=(dag_id, execution_date, records_count))
#     print(f"‚úÖ –ó–∞–ø–∏—Å—å –æ–± —É—Å–ø–µ—Ö–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ Postgres (ID: {dag_id}).")


def main(**kwargs):
    asyncio.run(run_real_ingestion(kwargs))